{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f235e7",
   "metadata": {},
   "source": [
    "# Lightning Ray\n",
    "\n",
    "In this notebook, we perform a basic transformer classification task. \n",
    "The main purpose is exploration of PyTorch Lightning and Ray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae07de5",
   "metadata": {},
   "source": [
    "Lets start with a simple smoke test. We will perform an inference baseline on this machine with nothing added on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b379e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized shapes: {'input_ids': (8, 33), 'attention_mask': (8, 33)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference per batch 0.08 ms (batch_size) 8\n",
      "Average inference per batch 0.16 ms (batch_size) 8\n",
      "Average inference per batch 0.24 ms (batch_size) 8\n",
      "Average inference per batch 0.30 ms (batch_size) 8\n",
      "Average inference per batch 0.40 ms (batch_size) 8\n",
      "Average inference per batch 0.52 ms (batch_size) 8\n",
      "Average inference per batch 0.64 ms (batch_size) 8\n",
      "Average inference per batch 0.78 ms (batch_size) 8\n",
      "Average inference per batch 0.92 ms (batch_size) 8\n",
      "Average inference per batch 1.06 ms (batch_size) 8\n",
      "Average inference per batch 1.20 ms (batch_size) 8\n",
      "Average inference per batch 1.32 ms (batch_size) 8\n",
      "Average inference per batch 1.46 ms (batch_size) 8\n",
      "Average inference per batch 1.59 ms (batch_size) 8\n",
      "Average inference per batch 1.71 ms (batch_size) 8\n",
      "Average inference per batch 1.77 ms (batch_size) 8\n",
      "Average inference per batch 1.83 ms (batch_size) 8\n",
      "Average inference per batch 1.89 ms (batch_size) 8\n",
      "Average inference per batch 1.95 ms (batch_size) 8\n",
      "Average inference per batch 1.99 ms (batch_size) 8\n",
      "Average inference per batch 2.03 ms (batch_size) 8\n",
      "Average inference per batch 2.09 ms (batch_size) 8\n",
      "Average inference per batch 2.13 ms (batch_size) 8\n",
      "Average inference per batch 2.15 ms (batch_size) 8\n",
      "Average inference per batch 2.19 ms (batch_size) 8\n",
      "Average inference per batch 2.25 ms (batch_size) 8\n",
      "Average inference per batch 2.29 ms (batch_size) 8\n",
      "Average inference per batch 2.33 ms (batch_size) 8\n",
      "Average inference per batch 2.37 ms (batch_size) 8\n",
      "Average inference per batch 2.41 ms (batch_size) 8\n",
      "Average inference per batch 2.45 ms (batch_size) 8\n",
      "Average inference per batch 2.47 ms (batch_size) 8\n",
      "Average inference per batch 2.51 ms (batch_size) 8\n",
      "Average inference per batch 2.55 ms (batch_size) 8\n",
      "Average inference per batch 2.57 ms (batch_size) 8\n",
      "Average inference per batch 2.61 ms (batch_size) 8\n",
      "Average inference per batch 2.65 ms (batch_size) 8\n",
      "Average inference per batch 2.67 ms (batch_size) 8\n",
      "Average inference per batch 2.71 ms (batch_size) 8\n",
      "Average inference per batch 2.73 ms (batch_size) 8\n",
      "Average inference per batch 2.77 ms (batch_size) 8\n",
      "Average inference per batch 2.79 ms (batch_size) 8\n",
      "Average inference per batch 2.83 ms (batch_size) 8\n",
      "Average inference per batch 2.87 ms (batch_size) 8\n",
      "Average inference per batch 2.89 ms (batch_size) 8\n",
      "Average inference per batch 2.93 ms (batch_size) 8\n",
      "Average inference per batch 2.95 ms (batch_size) 8\n",
      "Average inference per batch 2.99 ms (batch_size) 8\n",
      "Average inference per batch 3.03 ms (batch_size) 8\n",
      "Average inference per batch 3.05 ms (batch_size) 8\n",
      "Smoke Test Complete\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, time\n",
    "\n",
    "MODEL_ID = \"sshleifer/tiny-distilroberta-base\" # Super small, OK on CPU\n",
    "ds = load_dataset(\"glue\", \"sst2\", split = \"train[:200]\") #small slice\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "batch = tok(list(ds[\"sentence\"][:8]),\n",
    "            padding = True,\n",
    "            truncation = True, \n",
    "            max_length=128,\n",
    "            return_tensors =\"pt\")\n",
    "\n",
    "print(\"Tokenized shapes:\", {k: tuple(v.shape) for k, v in batch.items()})\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels =2)\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _ = model(**batch) # warmup\n",
    "    iters = 50\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        _ = model(**batch)\n",
    "        dt = time.time() - t0\n",
    "        bs = batch[\"input_ids\"].shape[0]\n",
    "        print(f\"Average inference per batch {dt/iters*1000:.2f} ms (batch_size) {bs}\")\n",
    "        \n",
    "print(\"Smoke Test Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141c139",
   "metadata": {},
   "source": [
    "Lets introduce some lightning elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b06eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataModule Defined\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "MODEL_ID = \"sshleifer/tiny-distilroberta-base\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "class SST2DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, model_id = MODEL_ID, batch_size =32,num_workers=0, pin_memory=False, persistent_workers=False):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.collate = DataCollatorWithPadding(self.tokenizer)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # download/cache only\n",
    "        load_dataset(\"glue\", \"sst2\")\n",
    "        AutoTokenizer.from_pretrained(self.model_id)\n",
    "        \n",
    "    def setup(self, stage= None):\n",
    "        ds_train = load_dataset(\"glue\", \"sst2\", split=\"train[:1000]\")\n",
    "        ds_val = load_dataset(\"glue\", \"sst2\", split=\"validation[:200]\")\n",
    "        \n",
    "        def tok_fn(examples):\n",
    "            t = self.tokenizer(\n",
    "                examples[\"sentence\"],\n",
    "                truncation=True,\n",
    "                max_length=MAX_LEN,\n",
    "            )\n",
    "            t[\"labels\"] = examples[\"label\"]  # copy labels -> 'labels'\n",
    "            return t\n",
    "        \n",
    "        # batched tokenization\n",
    "        ds_train = ds_train.map(tok_fn, batched=True,  remove_columns=ds_train.column_names)\n",
    "        ds_val = ds_val.map(tok_fn, batched=True,  remove_columns=ds_val.column_names)    \n",
    "        \n",
    "        self.ds_train, self.ds_val = ds_train, ds_val\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,\n",
    "        )\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,\n",
    "        )\n",
    "        \n",
    "print(\"DataModule Defined\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902b5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightning Module Ready\n"
     ]
    }
   ],
   "source": [
    "#Now the lighnting module. This wraps the HF model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class LitTinyClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_id=MODEL_ID, lr=5e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, **batch):\n",
    "        return self.model(**batch)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(**batch)\n",
    "        loss = out.loss\n",
    "        \n",
    "        #Quick accuracy sanity check\n",
    "        preds = out.logits.argmax(dim=-1)\n",
    "        acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(**batch)\n",
    "        loss = out.loss\n",
    "        preds = out.logits.argmax(dim=-1)\n",
    "        acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "    \n",
    "print(\"Lightning Module Ready\")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc67d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 6GB Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 22728.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 5604.74 examples/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                             | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 101 K  | eval\n",
      "------------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "50        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:527: Found 50 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 19.61it/s, v_num=14, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 19.42it/s, v_num=14, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]\n"
     ]
    }
   ],
   "source": [
    "#Lets do just a single epoch of training\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "dm = SST2DataModule(\n",
    "    model_id=MODEL_ID,\n",
    "    batch_size=32,\n",
    "    num_workers=2,          # start at 2 on Windows\n",
    "    pin_memory=True,        # good for CUDA async H2D copies\n",
    "    persistent_workers=True # avoid respawn cost each epoch\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "model = LitTinyClassifier(model_id=MODEL_ID, lr = 5e-5)\n",
    "\n",
    "precision = \"bf16-mixed\" if hasattr(torch.cuda, \"is_available\") and torch.cuda.is_available() else \"32-true\"\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=precision,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891c28f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 baseline (bs=8): {'batch_size': 32, 'mean_ms': 3.455183334493389, 'p50_ms': 3.353599982801825, 'p95_ms': 4.365645052166655, 'p99_ms': 5.521776984678577, 'repeats': 150}\n",
      "FP32 baseline (bs=1): {'batch_size': 1, 'mean_ms': 1.7905913351569325, 'p50_ms': 1.812000060454011, 'p95_ms': 2.214680088218302, 'p99_ms': 2.840427967021241, 'repeats': 300}\n"
     ]
    }
   ],
   "source": [
    "# Inference latency\n",
    "import torch, numpy as np\n",
    "import time\n",
    "\n",
    "def _to_device(batch, device):\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def _should_sync(device: str) -> bool:\n",
    "    return device.startswith(\"cuda\") and torch.cuda.is_available()\n",
    "\n",
    "def measure_latency(hf_model, batch, repeats = 200, warmup = 20, device =\"cpu\"):\n",
    "    hf_model.eval().to(device)\n",
    "    batch = _to_device(batch, device)\n",
    "    \n",
    "    # warmup (not timed)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(warmup):\n",
    "            _ = hf_model(**batch)\n",
    "    if _should_sync(device):\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    # timed loop\n",
    "    samples_ms = []\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = hf_model(**batch)\n",
    "            if _should_sync(device):\n",
    "                torch.cuda.synchronize()\n",
    "            samples_ms.append((time.perf_counter() - t0) * 1000.0)\n",
    "\n",
    "    samples_ms = np.asarray(samples_ms, dtype=float)\n",
    "    return {\n",
    "        \"batch_size\": int(batch[\"input_ids\"].shape[0]),\n",
    "        \"mean_ms\": float(samples_ms.mean()),\n",
    "        \"p50_ms\": float(np.percentile(samples_ms, 50)),\n",
    "        \"p95_ms\": float(np.percentile(samples_ms, 95)),\n",
    "        \"p99_ms\": float(np.percentile(samples_ms, 99)),\n",
    "        \"repeats\": int(repeats),\n",
    "    }\n",
    "    \n",
    "# grab a validation batch\n",
    "val_loader = dm.val_dataloader()\n",
    "batch_val = next(iter(val_loader))\n",
    "\n",
    "# bs=8\n",
    "fp32_bs8 = measure_latency(model.model, batch_val, repeats=150, warmup=30, device=\"cpu\")\n",
    "\n",
    "# bs=1 (edge/onboard-ish)\n",
    "single = {k: v[:1].clone() for k, v in batch_val.items()}\n",
    "fp32_bs1 = measure_latency(model.model, single, repeats=300, warmup=50, device=\"cpu\")\n",
    "\n",
    "print(\"FP32 baseline (bs=8):\", fp32_bs8)\n",
    "print(\"FP32 baseline (bs=1):\", fp32_bs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcde7ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fiona\\AppData\\Local\\Temp\\ipykernel_45784\\810471590.py:3: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qmodel = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 quant (bs=8): {'batch_size': 32, 'mean_ms': 6.670532658851395, 'p50_ms': 5.942899966612458, 'p95_ms': 11.41486995620653, 'p99_ms': 11.903988005360587, 'repeats': 150}\n",
      "INT8 quant (bs=1): {'batch_size': 1, 'mean_ms': 3.0304009979590774, 'p50_ms': 2.9561500414274633, 'p95_ms': 3.862739959731698, 'p99_ms': 4.119967934675514, 'repeats': 300}\n",
      "Quick val accuracy FP32: 0.495 | INT8: 0.495\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "qmodel = torch.quantization.quantize_dynamic(\n",
    "    model.model, \n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "\n",
    "q_bs8 = measure_latency(qmodel, batch_val, repeats=150, warmup=30, device=\"cpu\")\n",
    "q_bs1 = measure_latency(qmodel, single,   repeats=300, warmup=50, device=\"cpu\")\n",
    "\n",
    "\n",
    "print(\"INT8 quant (bs=8):\", q_bs8)\n",
    "print(\"INT8 quant (bs=1):\", q_bs1)\n",
    "\n",
    "# quick accuracy sanity on a few batches\n",
    "def quick_accuracy(hf_model, loader, max_batches=10, device=\"cpu\"):\n",
    "    hf_model.eval().to(device)\n",
    "    correct = total = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, b in enumerate(loader):\n",
    "            if i >= max_batches: break\n",
    "            b = _to_device(b, device)\n",
    "            out = hf_model(**b)\n",
    "            preds = out.logits.argmax(dim=-1)\n",
    "            correct += (preds == b[\"labels\"]).sum().item()\n",
    "            total   += preds.numel()\n",
    "    return correct / total\n",
    "\n",
    "acc_fp32 = quick_accuracy(model.model, dm.val_dataloader(), max_batches=10)\n",
    "acc_int8 = quick_accuracy(qmodel,      dm.val_dataloader(), max_batches=10)\n",
    "print(f\"Quick val accuracy FP32: {acc_fp32:.3f} | INT8: {acc_int8:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8835b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataModule (CUDA-ready) defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 1 â€” CUDA-friendly DataModule (workers/pinning)\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# keep your existing MODEL_ID / MAX_LEN from before\n",
    "# MODEL_ID = \"...\" \n",
    "# MAX_LEN = 128\n",
    "\n",
    "class SST2DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, model_id=MODEL_ID, batch_size=32, num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=2):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.collate = DataCollatorWithPadding(self.tokenizer)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        load_dataset(\"glue\", \"sst2\")\n",
    "        AutoTokenizer.from_pretrained(self.model_id)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        ds_train = load_dataset(\"glue\", \"sst2\", split=\"train[:1000]\")\n",
    "        ds_val   = load_dataset(\"glue\", \"sst2\", split=\"validation[:200]\")\n",
    "\n",
    "        def tok_fn(batch):\n",
    "            t = self.tokenizer(batch[\"sentence\"], truncation=True, max_length=MAX_LEN)\n",
    "            t[\"labels\"] = batch[\"label\"]\n",
    "            return t\n",
    "\n",
    "        ds_train = ds_train.map(tok_fn, batched=True, remove_columns=ds_train.column_names)\n",
    "        ds_val   = ds_val.map(tok_fn,   batched=True, remove_columns=ds_val.column_names)\n",
    "        self.ds_train, self.ds_val = ds_train, ds_val\n",
    "\n",
    "    def _loader(self, ds, shuffle: bool):\n",
    "        kw = dict(\n",
    "            dataset=ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,\n",
    "        )\n",
    "        if self.num_workers > 0:\n",
    "            kw[\"prefetch_factor\"] = self.prefetch_factor\n",
    "        return DataLoader(**kw)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._loader(self.ds_train, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._loader(self.ds_val, shuffle=False)\n",
    "\n",
    "print(\"DataModule (CUDA-ready) defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e6964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 19980.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 4222.19 examples/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                             | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 101 K  | eval\n",
      "------------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "50        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 16.91it/s, v_num=15, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 16.78it/s, v_num=15, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” GPU training\n",
    "import pytorch_lightning as pl, torch\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "dm = SST2DataModule(model_id=MODEL_ID, batch_size=32, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "dm.prepare_data(); dm.setup()\n",
    "\n",
    "model = LitTinyClassifier(model_id=MODEL_ID, lr=5e-5)\n",
    "\n",
    "can_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "precision = \"bf16-mixed\" if can_bf16 else (\"16-mixed\" if torch.cuda.is_available() else \"32-true\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=precision,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccda7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA FP32 (bs=8): {'batch_size': 32, 'mean_ms': 2.531806997139938, 'p50_ms': 2.215099986642599, 'p95_ms': 4.532704927260055, 'p99_ms': 5.962219965877008, 'repeats': 200, 'amp': False, 'device': 'cuda'}\n",
      "CUDA AMP  (bs=8): {'batch_size': 32, 'mean_ms': 3.429909997503273, 'p50_ms': 3.087949939072132, 'p95_ms': 5.425290024140846, 'p99_ms': 6.7342530004680095, 'repeats': 200, 'amp': True, 'device': 'cuda'}\n",
      "CUDA FP32 (bs=1): {'batch_size': 1, 'mean_ms': 2.2018830000888556, 'p50_ms': 2.050450013484806, 'p95_ms': 3.317129996139556, 'p99_ms': 4.01725108618848, 'repeats': 300, 'amp': False, 'device': 'cuda'}\n",
      "CUDA AMP  (bs=1): {'batch_size': 1, 'mean_ms': 3.0379619992648563, 'p50_ms': 2.707950014155358, 'p95_ms': 5.167229997459799, 'p99_ms': 6.680601987754925, 'repeats': 300, 'amp': True, 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” CUDA latency\n",
    "import time, numpy as np, torch\n",
    "\n",
    "def _to_device(batch, device):\n",
    "    return {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "def _should_sync(device: str) -> bool:\n",
    "    return device.startswith(\"cuda\") and torch.cuda.is_available()\n",
    "\n",
    "def measure_latency(hf_model, batch, repeats=200, warmup=60, device=\"cuda\", amp=False):\n",
    "    hf_model.eval().to(device)\n",
    "    batch = _to_device(batch, device)\n",
    "\n",
    "    # warmup\n",
    "    with torch.inference_mode():\n",
    "        if amp and device.startswith(\"cuda\"):\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)):\n",
    "                for _ in range(warmup): _ = hf_model(**batch)\n",
    "        else:\n",
    "            for _ in range(warmup): _ = hf_model(**batch)\n",
    "    if _should_sync(device): torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            if amp and device.startswith(\"cuda\"):\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)):\n",
    "                    _ = hf_model(**batch)\n",
    "            else:\n",
    "                _ = hf_model(**batch)\n",
    "            if _should_sync(device): torch.cuda.synchronize()\n",
    "            times.append((time.perf_counter() - t0) * 1000.0)\n",
    "\n",
    "    arr = np.asarray(times, dtype=float)\n",
    "    return {\n",
    "        \"batch_size\": int(batch[\"input_ids\"].shape[0]),\n",
    "        \"mean_ms\": float(arr.mean()),\n",
    "        \"p50_ms\": float(np.percentile(arr, 50)),\n",
    "        \"p95_ms\": float(np.percentile(arr, 95)),\n",
    "        \"p99_ms\": float(np.percentile(arr, 99)),\n",
    "        \"repeats\": int(repeats),\n",
    "        \"amp\": bool(amp),\n",
    "        \"device\": device,\n",
    "    }\n",
    "\n",
    "# grab a val batch\n",
    "val_loader = dm.val_dataloader()\n",
    "batch_val = next(iter(val_loader))\n",
    "\n",
    "# bs=8\n",
    "fp32_cuda_bs8 = measure_latency(model.model, batch_val, repeats=200, warmup=80, device=\"cuda\", amp=False)\n",
    "amp_cuda_bs8  = measure_latency(model.model, batch_val, repeats=200, warmup=80, device=\"cuda\", amp=True)\n",
    "\n",
    "# bs=1\n",
    "single = {k: v[:1].clone() for k, v in batch_val.items()}\n",
    "fp32_cuda_bs1 = measure_latency(model.model, single, repeats=300, warmup=100, device=\"cuda\", amp=False)\n",
    "amp_cuda_bs1  = measure_latency(model.model, single, repeats=300, warmup=100, device=\"cuda\", amp=True)\n",
    "\n",
    "print(\"CUDA FP32 (bs=8):\", fp32_cuda_bs8)\n",
    "print(\"CUDA AMP  (bs=8):\", amp_cuda_bs8)\n",
    "print(\"CUDA FP32 (bs=1):\", fp32_cuda_bs1)\n",
    "print(\"CUDA AMP  (bs=1):\", amp_cuda_bs1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c6498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Parameter 'function'=<function SST2DataModule.setup.<locals>.tok_fn at 0x0000024A0FAAE020> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 19173.44 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 7110.44 examples/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                             | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 101 K  | eval\n",
      "------------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "50        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  9.13it/s, v_num=16, train_loss=0.693, train_acc=0.548]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  9.04it/s, v_num=16, train_loss=0.693, train_acc=0.548]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "Profile stats for: records\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us     124.543ms       277.43%     124.543ms      15.568ms           0 B           0 B           0 B           0 B             8  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.training...         2.17%      11.381ms        20.11%     105.460ms      13.182ms       0.000us         0.00%      18.892ms       2.361ms           0 B           0 B       9.99 MB     -52.00 KB             8  \n",
      "[pl][module]transformers.models.roberta.modeling_rob...         0.33%       1.744ms        17.09%      89.622ms      11.203ms       0.000us         0.00%      18.255ms       2.282ms           0 B           0 B       9.99 MB     -90.00 KB             8  \n",
      "[pl][module]transformers.models.roberta.modeling_rob...         0.76%       4.009ms        15.42%      80.860ms      10.107ms       0.000us         0.00%      17.733ms       2.217ms           0 B         -32 B      10.05 MB      -6.43 MB             8  \n",
      "                        [pl][profile]run_training_batch         0.21%       1.089ms        37.31%     195.683ms      24.460ms       0.000us         0.00%      16.270ms       2.034ms           0 B           0 B       1.37 MB      -3.50 KB             8  \n",
      "[pl][profile][LightningModule]LitTinyClassifier.opti...         0.06%     307.600us        37.09%     194.535ms      24.317ms       0.000us         0.00%      16.270ms       2.034ms           0 B           0 B       1.38 MB           0 B             8  \n",
      "                              Optimizer.step#AdamW.step        16.88%      88.561ms        37.02%     194.190ms      24.274ms       0.000us         0.00%      16.270ms       2.034ms           0 B           0 B       1.38 MB      -8.61 MB             8  \n",
      "[pl][module]transformers.models.roberta.modeling_rob...         0.19%     976.800us        11.15%      58.508ms       7.313ms       0.000us         0.00%      14.472ms       1.809ms           0 B           0 B       9.78 MB     -90.00 KB             8  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.training...         0.00%       0.000us         0.00%       0.000us       0.000us      14.330ms        31.92%      14.330ms       1.791ms           0 B           0 B           0 B           0 B             8  \n",
      "[pl][module]transformers.models.roberta.modeling_rob...         0.00%       0.000us         0.00%       0.000us       0.000us      12.443ms        27.72%      12.443ms       1.555ms           0 B           0 B           0 B           0 B             8  \n",
      "[pl][module]transformers.models.roberta.modeling_rob...         0.00%       0.000us         0.00%       0.000us       0.000us       9.451ms        21.05%       9.451ms       1.181ms           0 B           0 B           0 B           0 B             8  \n",
      "                                          ProfilerStep*        20.23%     106.108ms        50.68%     265.842ms      33.230ms       0.000us         0.00%       8.960ms       1.120ms           0 B          32 B     535.00 KB       5.18 MB             8  \n",
      "                                       aten::layer_norm         0.05%     277.700us         0.82%       4.278ms     106.945us       0.000us         0.00%       8.724ms     218.105us           0 B           0 B     910.00 KB           0 B            40  \n",
      "                                aten::native_layer_norm         0.39%       2.062ms         0.76%       4.000ms     100.003us       8.724ms        19.43%       8.724ms     218.105us           0 B           0 B     910.00 KB           0 B            40  \n",
      "                     aten::scaled_dot_product_attention         0.05%     270.600us         4.13%      21.689ms     677.781us       0.000us         0.00%       8.434ms     263.556us           0 B           0 B      17.44 MB      -5.20 MB            32  \n",
      "[pl][module]torch.nn.modules.linear.Linear: model.ro...         0.00%       0.000us         0.00%       0.000us       0.000us       8.195ms        18.25%       8.195ms       1.024ms           0 B           0 B           0 B           0 B             8  \n",
      "_ZN2at6native53_GLOBAL__N__87a90936_20_layer_norm_ke...         0.00%       0.000us         0.00%       0.000us       0.000us       7.780ms        17.33%       7.780ms     194.498us           0 B           0 B           0 B           0 B            40  \n",
      "[pl][module]transformers.models.roberta.modeling_rob...         0.21%       1.088ms         5.23%      27.458ms       3.432ms       0.000us         0.00%       7.336ms     917.004us           0 B           0 B       4.93 MB     -90.00 KB             8  \n",
      "                                            aten::copy_         1.53%       8.012ms         5.11%      26.809ms      24.823us       7.139ms        15.90%       7.139ms       6.611us           0 B           0 B           0 B           0 B          1080  \n",
      "[pl][module]transformers.models.roberta.modeling_rob...         0.23%       1.219ms         5.73%      30.073ms       3.759ms       0.000us         0.00%       7.136ms     892.026us           0 B           0 B       4.93 MB     -90.00 KB             8  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 524.544ms\n",
      "Self CUDA time total: 44.891ms\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB trace dir: c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\tb_traces\n",
      "Contents: ['fionan_45784.1757258988193973400.pt.trace.json']\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, pytorch_lightning as pl, torch\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "from torch.profiler import ProfilerActivity, schedule, tensorboard_trace_handler\n",
    "\n",
    "# clean old traces so we know weâ€™re seeing fresh ones\n",
    "shutil.rmtree(\"tb_traces\", ignore_errors=True)\n",
    "os.makedirs(\"tb_traces\", exist_ok=True)\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "dm = SST2DataModule(model_id=MODEL_ID, batch_size=32, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "dm.prepare_data(); dm.setup()\n",
    "model = LitTinyClassifier(model_id=MODEL_ID, lr=5e-5)\n",
    "\n",
    "profiler = PyTorchProfiler(\n",
    "    schedule=schedule(wait=1, warmup=1, active=8, repeat=1),   # short & guaranteed to run\n",
    "    activities=[ProfilerActivity.CPU] + ([ProfilerActivity.CUDA] if torch.cuda.is_available() else []),\n",
    "    on_trace_ready=tensorboard_trace_handler(\"tb_traces\"),      # <-- write TensorBoard event files\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=0.7,     # ensure we surpass wait+warmup+active\n",
    "    limit_val_batches=0,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=(\"bf16-mixed\" if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "              else (\"16-mixed\" if torch.cuda.is_available() else \"32-true\")),\n",
    "    profiler=profiler,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)\n",
    "\n",
    "print(\"TB trace dir:\", os.path.abspath(\"tb_traces\"))\n",
    "print(\"Contents:\", os.listdir(\"tb_traces\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b59a742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 43892), started 6:34:05 ago. (Use '!kill 43892' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3eb13b9046685257\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3eb13b9046685257\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_traces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74996a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 15696), started 0:00:12 ago. (Use '!kill 15696' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bd9c66b3ad3c2d6d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bd9c66b3ad3c2d6d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace 43892 with the PID shown in your message if different\n",
    "!taskkill /PID 43892 /F  2> NUL\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir tb_traces --port 6007 --reload_interval 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3774e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORT version: 1.22.1\n",
      "Available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "CPU count: 20\n"
     ]
    }
   ],
   "source": [
    "# Time to investigate ONNX runtime\n",
    "import onnxruntime as ort, os, multiprocessing as mp\n",
    "print(\"ORT version:\", ort.__version__)\n",
    "print(\"Available providers:\", ort.get_available_providers())\n",
    "print(\"CPU count:\", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88d326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fiona\\AppData\\Local\\Temp\\ipykernel_45784\\290519221.py:18: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORT CPU batch: {'bs': 32, 'mean_ms': 2.7413950028130785, 'p50_ms': 2.685900020878762, 'p95_ms': 3.5613349929917604}\n",
      "ORT CPU bs=1 : {'bs': 1, 'mean_ms': 0.29227467253804207, 'p50_ms': 0.27864996809512377, 'p95_ms': 0.3899000585079193}\n"
     ]
    }
   ],
   "source": [
    "import torch, time, numpy as np\n",
    "import onnxruntime as ort\n",
    "from copy import deepcopy\n",
    "\n",
    "onnx_path = \"tiny_roberta_sst2.onnx\"\n",
    "hf = deepcopy(model.model).eval().to(\"cpu\")\n",
    "\n",
    "val_loader = dm.val_dataloader()\n",
    "batch_val = next(iter(val_loader))\n",
    "batch_nolabel = {k: v for k, v in batch_val.items() if k != \"labels\"}\n",
    "\n",
    "example = (batch_nolabel[\"input_ids\"], batch_nolabel[\"attention_mask\"])\n",
    "dynamic_axes = {\"input_ids\": {0:\"batch\",1:\"seq\"},\n",
    "                \"attention_mask\": {0:\"batch\",1:\"seq\"},\n",
    "                \"logits\": {0:\"batch\"}}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        hf, example, onnx_path,\n",
    "        input_names=[\"input_ids\",\"attention_mask\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=17, do_constant_folding=True\n",
    "    )\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "sess = ort.InferenceSession(onnx_path, sess_options=so, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "import numpy as np, time\n",
    "def to_numpy(t): return t.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "def ort_latency(session, batch, repeats=200, warmup=20):\n",
    "    inputs = {\"input_ids\": to_numpy(batch[\"input_ids\"]),\n",
    "              \"attention_mask\": to_numpy(batch[\"attention_mask\"])}\n",
    "    for _ in range(warmup): _ = session.run([\"logits\"], inputs)\n",
    "    ts=[]\n",
    "    for _ in range(repeats):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = session.run([\"logits\"], inputs)\n",
    "        ts.append((time.perf_counter()-t0)*1000)\n",
    "    arr=np.asarray(ts,float)\n",
    "    return {\"bs\": int(inputs[\"input_ids\"].shape[0]),\n",
    "            \"mean_ms\": float(arr.mean()),\n",
    "            \"p50_ms\": float(np.percentile(arr,50)),\n",
    "            \"p95_ms\": float(np.percentile(arr,95))}\n",
    "    \n",
    "# batch = full val batch; single = bs=1\n",
    "single = {k: v[:1].clone() for k, v in batch_nolabel.items()}\n",
    "print(\"ORT CPU batch:\", ort_latency(sess, batch_nolabel))\n",
    "print(\"ORT CPU bs=1 :\", ort_latency(sess, single, repeats=300, warmup=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ce86782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threads=1 â†’ bs=1 p50=0.103ms, p95=0.115ms\n",
      "threads=2 â†’ bs=1 p50=0.127ms, p95=0.147ms\n",
      "threads=4 â†’ bs=1 p50=0.141ms, p95=0.258ms\n",
      "threads=8 â†’ bs=1 p50=0.192ms, p95=0.317ms\n"
     ]
    }
   ],
   "source": [
    "# ONNX-3 â€” vary threads and see what happens\n",
    "import onnxruntime as ort, numpy as np\n",
    "\n",
    "def make_session(num_threads):\n",
    "    so = ort.SessionOptions()\n",
    "    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    so.intra_op_num_threads = num_threads   # math inside an op\n",
    "    so.inter_op_num_threads = 1            # and across ops; keep 1 for latency\n",
    "    return ort.InferenceSession(onnx_path, sess_options=so, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "for n in [1, 2, 4, 8]:\n",
    "    s = make_session(n)\n",
    "    stats = ort_latency(s, single, repeats=300, warmup=50)\n",
    "    print(f\"threads={n} â†’ bs=1 p50={stats['p50_ms']:.3f}ms, p95={stats['p95_ms']:.3f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce41c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 18:33:55,108\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-07 18:33:57,510\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray: 2.49.1\n",
      "Python: 3.12.10 | OS: Windows-11-10.0.26100-SP0\n",
      "Resources detected: {'CPU': 20.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 717299712.0, 'memory': 1673699328.0}\n",
      "Round-trip OK: {'hello': 'ray'}\n"
     ]
    }
   ],
   "source": [
    "# RAY-1: init & resources\n",
    "import ray, platform, sys, os\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
    "\n",
    "print(\"Ray:\", ray.__version__)\n",
    "print(\"Python:\", sys.version.split()[0], \"| OS:\", platform.platform())\n",
    "print(\"Resources detected:\", ray.cluster_resources())\n",
    "\n",
    "# tiny sanity: put/get round-trip through Ray's object store\n",
    "obj_ref = ray.put({\"hello\": \"ray\"})\n",
    "print(\"Round-trip OK:\", ray.get(obj_ref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c281fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 18:35:42,305\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 results: [(0, 26980), (1, 12204), (4, 21172), (9, 28476), (16, 40692)]\n",
      "Wall time for 20 Ã— 0.2s tasks: 0.33s\n"
     ]
    }
   ],
   "source": [
    "import ray, time, os\n",
    "ray.shutdown(); ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
    "\n",
    "@ray.remote\n",
    "def slow_square(x):\n",
    "    time.sleep(0.2)         # simulate work\n",
    "    return (x * x, os.getpid())\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "futs = [slow_square.remote(i) for i in range(20)]\n",
    "vals = ray.get(futs)\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"First 5 results:\", vals[:5])\n",
    "print(f\"Wall time for 20 Ã— 0.2s tasks: {t1 - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b503ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 18:36:40,884\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "\u001b[36m(InferenceActor pid=40016)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(InferenceActor pid=40016)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5002333521842957, 0.49976664781570435], [0.5002333521842957, 0.49976664781570435]]\n"
     ]
    }
   ],
   "source": [
    "import ray, torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "ray.shutdown(); ray.init(ignore_reinit_error=True)\n",
    "\n",
    "@ray.remote(num_gpus=1)   # reserve your single GPU\n",
    "class InferenceActor:\n",
    "    def __init__(self, model_id, max_len=128):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2).to(self.device).eval()\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # warmup once to trigger kernel autotuning/caches\n",
    "        with torch.inference_mode():\n",
    "            toks = self.tok([\"warm up\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_len).to(self.device)\n",
    "            _ = self.model(**toks)\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        with torch.inference_mode():\n",
    "            toks = self.tok(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_len).to(self.device)\n",
    "            logits = self.model(**toks).logits\n",
    "            return logits.softmax(dim=-1).tolist()\n",
    "\n",
    "actor = InferenceActor.remote(MODEL_ID)\n",
    "\n",
    "# single call\n",
    "probs = ray.get(actor.predict.remote([\"this is great!\", \"this is terrible...\"]))\n",
    "print(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42cd41ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5002333521842957, 0.49976664781570435]], [[0.5002333521842957, 0.49976664781570435]], [[0.5002333521842957, 0.49976664781570435]], [[0.5002333521842957, 0.49976664781570435]], [[0.5002333521842957, 0.49976664781570435]], [[0.5002333521842957, 0.49976664781570435]], [[0.5002333521842957, 0.49976664781570435]]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [[\"ok\"], [\"meh\"], [\"amazing!\"], [\"awful...\"], [\"fine\"], [\"great\"], [\"bad\"]]\n",
    "futs = [actor.predict.remote(s) for s in sentences]\n",
    "print([ray.get(f) for f in futs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a820c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 18:37:07,605\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-07 18:37:07,709\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-07 18:37:09,977\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "2025-09-07 18:37:11,500\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-07 18:37:11 (running for 00:00:00.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/fiona/AppData/Local/Temp/ray/session_2025-09-07_18-37-08_270046_45784/artifacts/2025-09-07_18-37-11/TorchTrainer_2025-09-07_18-37-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-07 18:37:16 (running for 00:00:05.23)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/fiona/AppData/Local/Temp/ray/session_2025-09-07_18-37-08_270046_45784/artifacts/2025-09-07_18-37-11/TorchTrainer_2025-09-07_18-37-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-07 18:37:21 (running for 00:00:10.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/fiona/AppData/Local/Temp/ray/session_2025-09-07_18-37-08_270046_45784/artifacts/2025-09-07_18-37-11/TorchTrainer_2025-09-07_18-37-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=11272)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=11272)\u001b[0m [W907 18:37:21.000000000 socket.cpp:755] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:51486 (system error: 10049 - The requested address is not valid in its context.).\n",
      "2025-09-07 18:37:22,132\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_48f84_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\worker.py\", line 2882, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\worker.py\", line 968, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=42752, ip=127.0.0.1, actor_id=a384f1997801f880aa58d98701000000, repr=TorchTrainer)\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1936, in ray._raylet.execute_task\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1876, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 689, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 461, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\air\\_internal\\util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 461, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\base_trainer.py\", line 883, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config)\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 261, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\base_trainer.py\", line 123, in _train_coordinator_fn\n",
      "    trainer.training_loop()\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\data_parallel_trainer.py\", line 458, in training_loop\n",
      "    backend_executor.start()\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\_internal\\backend_executor.py\", line 214, in start\n",
      "    self._backend.on_start(self.worker_group, self._backend_config)\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\torch\\config.py\", line 201, in on_start\n",
      "    ray.get(setup_futures)\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\worker.py\", line 2882, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\worker.py\", line 968, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute._setup_torch_process_group()\u001b[39m (pid=11272, ip=127.0.0.1, actor_id=9bee55214a9e8113f5068a5d01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x000001C4642557F0>)\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1936, in ray._raylet.execute_task\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1876, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 689, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 461, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 30, in __execute\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\torch\\config.py\", line 116, in _setup_torch_process_group\n",
      "    dist.init_process_group(\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\c10d_logger.py\", line 81, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\c10d_logger.py\", line 95, in wrapper\n",
      "    func_return = func(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 1764, in init_process_group\n",
      "    default_pg, _ = _new_process_group_helper(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 1999, in _new_process_group_helper\n",
      "    raise RuntimeError(\"Distributed package doesn't have NCCL built in\")\n",
      "RuntimeError: Distributed package doesn't have NCCL built in\n",
      "2025-09-07 18:37:22,161\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/fiona/ray_results/TorchTrainer_2025-09-07_18-37-11' in 0.0080s.\n",
      "2025-09-07 18:37:22,173\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_48f84_00000]\n",
      "2025-09-07 18:37:22,174\tINFO tune.py:1041 -- Total run time: 10.67 seconds (10.60 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-07 18:37:22 (running for 00:00:10.61)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/fiona/AppData/Local/Temp/ray/session_2025-09-07_18-37-08_270046_45784/artifacts/2025-09-07_18-37-11/TorchTrainer_2025-09-07_18-37-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "Number of errored trials: 1\n",
      "+--------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name               |   # failures | error file                                                                                                                                                                                                              |\n",
      "|--------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| TorchTrainer_48f84_00000 |            1 | C:/Users/fiona/AppData/Local/Temp/ray/session_2025-09-07_18-37-08_270046_45784/artifacts/2025-09-07_18-37-11/TorchTrainer_2025-09-07_18-37-11/driver_artifacts/TorchTrainer_48f84_00000_0_2025-09-07_18-37-11/error.txt |\n",
      "+--------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"C:/Users/fiona/ray_results/TorchTrainer_2025-09-07_18-37-11\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(RuntimeError)\u001b[39m                Traceback (most recent call last)",
      "\u001b[31mRayTaskError(RuntimeError)\u001b[39m: \u001b[36mray::_Inner.train()\u001b[39m (pid=42752, ip=127.0.0.1, actor_id=a384f1997801f880aa58d98701000000, repr=TorchTrainer)\n  File \"python\\\\ray\\\\_raylet.pyx\", line 1936, in ray._raylet.execute_task\n  File \"python\\\\ray\\\\_raylet.pyx\", line 1876, in ray._raylet.execute_task.function_executor\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 689, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 461, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 331, in train\n    raise skipped from exception_cause(skipped)\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\air\\_internal\\util.py\", line 107, in run\n    self._ret = self._target(*self._args, **self._kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 45, in <lambda>\n    training_func=lambda: self._trainable_func(self.config),\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 461, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\base_trainer.py\", line 883, in _trainable_func\n    super()._trainable_func(self._merged_config)\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 261, in _trainable_func\n    output = fn()\n             ^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\base_trainer.py\", line 123, in _train_coordinator_fn\n    trainer.training_loop()\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\data_parallel_trainer.py\", line 458, in training_loop\n    backend_executor.start()\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\_internal\\backend_executor.py\", line 214, in start\n    self._backend.on_start(self.worker_group, self._backend_config)\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\torch\\config.py\", line 201, in on_start\n    ray.get(setup_futures)\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 104, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\worker.py\", line 2882, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\worker.py\", line 968, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute._setup_torch_process_group()\u001b[39m (pid=11272, ip=127.0.0.1, actor_id=9bee55214a9e8113f5068a5d01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x000001C4642557F0>)\n  File \"python\\\\ray\\\\_raylet.pyx\", line 1936, in ray._raylet.execute_task\n  File \"python\\\\ray\\\\_raylet.pyx\", line 1876, in ray._raylet.execute_task.function_executor\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 689, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 461, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 33, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 30, in __execute\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\torch\\config.py\", line 116, in _setup_torch_process_group\n    dist.init_process_group(\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\c10d_logger.py\", line 81, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\c10d_logger.py\", line 95, in wrapper\n    func_return = func(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 1764, in init_process_group\n    default_pg, _ = _new_process_group_helper(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 1999, in _new_process_group_helper\n    raise RuntimeError(\"Distributed package doesn't have NCCL built in\")\nRuntimeError: Distributed package doesn't have NCCL built in",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTrainingFailedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(trainer.callback_metrics.get(\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m))}\n\u001b[32m     24\u001b[39m tt = TorchTrainer(\n\u001b[32m     25\u001b[39m     train_loop_per_worker=train_loop,\n\u001b[32m     26\u001b[39m     scaling_config=ScalingConfig(num_workers=\u001b[32m1\u001b[39m, use_gpu=torch.cuda.is_available()),\n\u001b[32m     27\u001b[39m     train_loop_config={\u001b[33m\"\u001b[39m\u001b[33mmodel_id\u001b[39m\u001b[33m\"\u001b[39m: MODEL_ID},\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m result = \u001b[43mtt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRay Train result:\u001b[39m\u001b[33m\"\u001b[39m, result)\n\u001b[32m     32\u001b[39m ray.shutdown()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\ray\\train\\base_trainer.py:722\u001b[39m, in \u001b[36mBaseTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    718\u001b[39m result = result_grid[\u001b[32m0\u001b[39m]\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[32m    721\u001b[39m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[32m    723\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([restore_msg, TrainingFailedError._FAILURE_CONFIG_MSG])\n\u001b[32m    724\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mresult\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mTrainingFailedError\u001b[39m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"C:/Users/fiona/ray_results/TorchTrainer_2025-09-07_18-37-11\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "import ray, torch, pytorch_lightning as pl\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "ray.shutdown(); ray.init(ignore_reinit_error=True)\n",
    "\n",
    "def train_loop(config):\n",
    "    import pytorch_lightning as pl, torch\n",
    "    pl.seed_everything(42, workers=True)\n",
    "    dm = SST2DataModule(model_id=config[\"model_id\"], batch_size=32, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "    dm.prepare_data(); dm.setup()\n",
    "    model = LitTinyClassifier(model_id=config[\"model_id\"], lr=5e-5)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        precision=\"16-mixed\" if torch.cuda.is_available() else \"32-true\",\n",
    "        log_every_n_steps=50,\n",
    "    )\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "    return {\"val_acc\": float(trainer.callback_metrics.get(\"val_acc\", 0.0))}\n",
    "\n",
    "tt = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    scaling_config=ScalingConfig(num_workers=1, use_gpu=torch.cuda.is_available()),\n",
    "    train_loop_config={\"model_id\": MODEL_ID},\n",
    ")\n",
    "result = tt.fit()\n",
    "print(\"Ray Train result:\", result)\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92abb8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found runs: ['C:\\\\Users\\\\fiona\\\\ray_results\\\\TorchTrainer_2025-09-07_18-37-11']\n",
      "Using: C:\\Users\\fiona\\ray_results\\TorchTrainer_2025-09-07_18-37-11\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pathlib, textwrap\n",
    "\n",
    "run_dir = r\"C:\\Users\\fiona\\ray_results\"  # base Ray results dir\n",
    "# pick the most recent TorchTrainer run automatically:\n",
    "cands = sorted(glob.glob(os.path.join(run_dir, \"TorchTrainer_*\")), key=os.path.getmtime)\n",
    "print(\"Found runs:\", cands[-3:])\n",
    "trial_dir = cands[-1]\n",
    "print(\"Using:\", trial_dir)\n",
    "\n",
    "# list interesting files\n",
    "for p in sorted(glob.glob(os.path.join(trial_dir, \"**\"), recursive=True)):\n",
    "    if any(s in p for s in (\".log\", \".err\")):\n",
    "        print(\" -\", p)\n",
    "\n",
    "# print the last 200 lines of each .err / driver log\n",
    "for p in sorted(glob.glob(os.path.join(trial_dir, \"**\", \"*.err\"), recursive=True)) + \\\n",
    "         sorted(glob.glob(os.path.join(trial_dir, \"driver_*\"))):\n",
    "    print(\"\\n==== tail:\", p, \"====\")\n",
    "    try:\n",
    "        with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            tail = f.readlines()[-200:]\n",
    "            print(\"\".join(tail))\n",
    "    except Exception as e:\n",
    "        print(\"Could not read:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c7289e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 18:39:47,242\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m Seed set to 42\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 20124.87 examples/s]\n",
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 5076.90 examples/s]\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3050 6GB Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 19522.83 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 6228.36 examples/s]\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m \n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m   | Name  | Type                             | Params | Mode\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m ------------------------------------------------------------------\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m 0 | model | RobertaForSequenceClassification | 101 K  | eval\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m ------------------------------------------------------------------\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m 101 K     Trainable params\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m 101 K     Total params\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m 0.407     Total estimated model params size (MB)\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m 0         Modules in train mode\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m 50        Modules in eval mode\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:527: Found 50 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s]                            \n",
      "Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:01, 21.52it/s, v_num=17]\n",
      "Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:00<00:00, 32.81it/s, v_num=17]\n",
      "Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 38.41it/s, v_num=17]\n",
      "Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 39.53it/s, v_num=17]\n",
      "Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:00<00:00, 41.13it/s, v_num=17]\n",
      "Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:00<00:00, 43.23it/s, v_num=17]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 44.04it/s, v_num=17]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:00, 196.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:00, 141.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:00<00:00, 135.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:00<00:00, 130.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:00<00:00, 133.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:00<00:00, 131.03it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 132.05it/s]\u001b[A\n",
      "\u001b[36m(LightningTrainActor pid=21972)\u001b[0m \n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 40.19it/s, v_num=17, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]\n",
      "Ray actor training result: {'val_acc': 0.4950000047683716, 'val_loss': 0.6933789253234863, 'elapsed_s': 13.721629999927245, 'precision': 'bf16-mixed'}\n"
     ]
    }
   ],
   "source": [
    "import ray, torch, pytorch_lightning as pl\n",
    "\n",
    "ray.shutdown(); ray.init(ignore_reinit_error=True)\n",
    "\n",
    "@ray.remote(num_gpus=1)  # reserve your single GPU\n",
    "class LightningTrainActor:\n",
    "    def __init__(self):\n",
    "        import pytorch_lightning as pl\n",
    "        pl.seed_everything(42, workers=True)\n",
    "\n",
    "    def train_once(self, model_id, batch_size=32, precision=None):\n",
    "        import pytorch_lightning as pl, torch, time\n",
    "\n",
    "        # Data\n",
    "        dm = SST2DataModule(model_id=model_id, batch_size=batch_size,\n",
    "                            num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "        dm.prepare_data(); dm.setup()\n",
    "\n",
    "        # Model\n",
    "        model = LitTinyClassifier(model_id=model_id, lr=5e-5)\n",
    "\n",
    "        # Precision\n",
    "        if precision is None:\n",
    "            precision = (\"bf16-mixed\" if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "                         else (\"16-mixed\" if torch.cuda.is_available() else \"32-true\"))\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=1,\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            devices=1,\n",
    "            precision=precision,\n",
    "            log_every_n_steps=25,\n",
    "        )\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        trainer.fit(model, datamodule=dm)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "\n",
    "        # capture a couple metrics\n",
    "        val_acc = float(trainer.callback_metrics.get(\"val_acc\", 0.0))\n",
    "        val_loss = float(trainer.callback_metrics.get(\"val_loss\", 0.0))\n",
    "        return {\"val_acc\": val_acc, \"val_loss\": val_loss, \"elapsed_s\": elapsed, \"precision\": precision}\n",
    "\n",
    "# spin up the actor and run training\n",
    "actor = LightningTrainActor.remote()\n",
    "result = ray.get(actor.train_once.remote(MODEL_ID, batch_size=32))\n",
    "print(\"Ray actor training result:\", result)\n",
    "\n",
    "ray.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 venv",
   "language": "python",
   "name": "pt312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
