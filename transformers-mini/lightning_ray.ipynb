{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f235e7",
   "metadata": {},
   "source": [
    "# Lightning Ray\n",
    "\n",
    "In this notebook, we perform a basic transformer classification task. \n",
    "The main purpose is exploration of PyTorch Lightning and Ray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae07de5",
   "metadata": {},
   "source": [
    "Lets start with a simple smoke test. We will perform an inference baseline on this machine with nothing added on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b379e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized shapes: {'input_ids': (8, 33), 'attention_mask': (8, 33)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference per batch 0.02 ms (batch_size) 8\n",
      "Average inference per batch 0.06 ms (batch_size) 8\n",
      "Average inference per batch 0.10 ms (batch_size) 8\n",
      "Average inference per batch 0.12 ms (batch_size) 8\n",
      "Average inference per batch 0.16 ms (batch_size) 8\n",
      "Average inference per batch 0.18 ms (batch_size) 8\n",
      "Average inference per batch 0.20 ms (batch_size) 8\n",
      "Average inference per batch 0.22 ms (batch_size) 8\n",
      "Average inference per batch 0.24 ms (batch_size) 8\n",
      "Average inference per batch 0.31 ms (batch_size) 8\n",
      "Average inference per batch 0.33 ms (batch_size) 8\n",
      "Average inference per batch 0.37 ms (batch_size) 8\n",
      "Average inference per batch 0.39 ms (batch_size) 8\n",
      "Average inference per batch 0.43 ms (batch_size) 8\n",
      "Average inference per batch 0.49 ms (batch_size) 8\n",
      "Average inference per batch 0.53 ms (batch_size) 8\n",
      "Average inference per batch 0.57 ms (batch_size) 8\n",
      "Average inference per batch 0.61 ms (batch_size) 8\n",
      "Average inference per batch 0.63 ms (batch_size) 8\n",
      "Average inference per batch 0.67 ms (batch_size) 8\n",
      "Average inference per batch 0.71 ms (batch_size) 8\n",
      "Average inference per batch 0.75 ms (batch_size) 8\n",
      "Average inference per batch 0.83 ms (batch_size) 8\n",
      "Average inference per batch 0.85 ms (batch_size) 8\n",
      "Average inference per batch 0.89 ms (batch_size) 8\n",
      "Average inference per batch 0.93 ms (batch_size) 8\n",
      "Average inference per batch 0.97 ms (batch_size) 8\n",
      "Average inference per batch 0.99 ms (batch_size) 8\n",
      "Average inference per batch 1.03 ms (batch_size) 8\n",
      "Average inference per batch 1.05 ms (batch_size) 8\n",
      "Average inference per batch 1.07 ms (batch_size) 8\n",
      "Average inference per batch 1.11 ms (batch_size) 8\n",
      "Average inference per batch 1.15 ms (batch_size) 8\n",
      "Average inference per batch 1.17 ms (batch_size) 8\n",
      "Average inference per batch 1.21 ms (batch_size) 8\n",
      "Average inference per batch 1.23 ms (batch_size) 8\n",
      "Average inference per batch 1.25 ms (batch_size) 8\n",
      "Average inference per batch 1.29 ms (batch_size) 8\n",
      "Average inference per batch 1.33 ms (batch_size) 8\n",
      "Average inference per batch 1.35 ms (batch_size) 8\n",
      "Average inference per batch 1.39 ms (batch_size) 8\n",
      "Average inference per batch 1.41 ms (batch_size) 8\n",
      "Average inference per batch 1.43 ms (batch_size) 8\n",
      "Average inference per batch 1.47 ms (batch_size) 8\n",
      "Average inference per batch 1.49 ms (batch_size) 8\n",
      "Average inference per batch 1.53 ms (batch_size) 8\n",
      "Average inference per batch 1.55 ms (batch_size) 8\n",
      "Average inference per batch 1.59 ms (batch_size) 8\n",
      "Average inference per batch 1.61 ms (batch_size) 8\n",
      "Average inference per batch 1.63 ms (batch_size) 8\n",
      "Smoke Test Complete\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, time\n",
    "\n",
    "MODEL_ID = \"sshleifer/tiny-distilroberta-base\" # Super small, OK on CPU\n",
    "ds = load_dataset(\"glue\", \"sst2\", split = \"train[:200]\") #small slice\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "batch = tok(list(ds[\"sentence\"][:8]),\n",
    "            padding = True,\n",
    "            truncation = True, \n",
    "            max_length=128,\n",
    "            return_tensors =\"pt\")\n",
    "\n",
    "print(\"Tokenized shapes:\", {k: tuple(v.shape) for k, v in batch.items()})\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels =2)\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _ = model(**batch) # warmup\n",
    "    iters = 50\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        _ = model(**batch)\n",
    "        dt = time.time() - t0\n",
    "        bs = batch[\"input_ids\"].shape[0]\n",
    "        print(f\"Average inference per batch {dt/iters*1000:.2f} ms (batch_size) {bs}\")\n",
    "        \n",
    "print(\"Smoke Test Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141c139",
   "metadata": {},
   "source": [
    "Lets introduce some lightning elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b06eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataModule Defined\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "MODEL_ID = \"sshleifer/tiny-distilroberta-base\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "class SST2DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, model_id = MODEL_ID, batch_size =32,num_workers=0, pin_memory=False, persistent_workers=False):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.collate = DataCollatorWithPadding(self.tokenizer)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # download/cache only\n",
    "        load_dataset(\"glue\", \"sst2\")\n",
    "        AutoTokenizer.from_pretrained(self.model_id)\n",
    "        \n",
    "    def setup(self, stage= None):\n",
    "        ds_train = load_dataset(\"glue\", \"sst2\", split=\"train[:1000]\")\n",
    "        ds_val = load_dataset(\"glue\", \"sst2\", split=\"validation[:200]\")\n",
    "        \n",
    "        def tok_fn(examples):\n",
    "            t = self.tokenizer(\n",
    "                examples[\"sentence\"],\n",
    "                truncation=True,\n",
    "                max_length=MAX_LEN,\n",
    "            )\n",
    "            t[\"labels\"] = examples[\"label\"]  # copy labels -> 'labels'\n",
    "            return t\n",
    "        \n",
    "        # batched tokenization\n",
    "        ds_train = ds_train.map(tok_fn, batched=True,  remove_columns=ds_train.column_names)\n",
    "        ds_val = ds_val.map(tok_fn, batched=True,  remove_columns=ds_val.column_names)    \n",
    "        \n",
    "        self.ds_train, self.ds_val = ds_train, ds_val\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,\n",
    "        )\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,\n",
    "        )\n",
    "        \n",
    "print(\"DataModule Defined\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902b5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightning Module Ready\n"
     ]
    }
   ],
   "source": [
    "#Now the lighnting module. This wraps the HF model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class LitTinyClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_id=MODEL_ID, lr=5e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, **batch):\n",
    "        return self.model(**batch)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self(**batch)\n",
    "        loss = out.loss\n",
    "        \n",
    "        #Quick accuracy sanity check\n",
    "        preds = out.logits.argmax(dim=-1)\n",
    "        acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(**batch)\n",
    "        loss = out.loss\n",
    "        preds = out.logits.argmax(dim=-1)\n",
    "        acc = (preds == batch[\"labels\"]).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "    \n",
    "print(\"Lightning Module Ready\")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc67d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17480.13 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 10243.00 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 6GB Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 21156.75 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 6818.45 examples/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                             | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 101 K  | eval\n",
      "------------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "50        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:527: Found 50 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 18.18it/s, v_num=2, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 18.00it/s, v_num=2, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]\n"
     ]
    }
   ],
   "source": [
    "#Lets do just a single epoch of training\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "dm = SST2DataModule(\n",
    "    model_id=MODEL_ID,\n",
    "    batch_size=32,\n",
    "    num_workers=2,          # start at 2 on Windows\n",
    "    pin_memory=True,        # good for CUDA async H2D copies\n",
    "    persistent_workers=True # avoid respawn cost each epoch\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "model = LitTinyClassifier(model_id=MODEL_ID, lr = 5e-5)\n",
    "\n",
    "precision = \"bf16-mixed\" if hasattr(torch.cuda, \"is_available\") and torch.cuda.is_available() else \"32-true\"\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=precision,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891c28f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 baseline (bs=8): {'batch_size': 32, 'mean_ms': 2.0493573353936276, 'p50_ms': 1.9735000096261501, 'p95_ms': 2.537545037921517, 'p99_ms': 2.7957989822607487, 'repeats': 150}\n",
      "FP32 baseline (bs=1): {'batch_size': 1, 'mean_ms': 1.238872332808872, 'p50_ms': 1.1840499937534332, 'p95_ms': 1.54692500946112, 'p99_ms': 2.0025459968019272, 'repeats': 300}\n"
     ]
    }
   ],
   "source": [
    "# Inference latency\n",
    "import torch, numpy as np\n",
    "import time\n",
    "\n",
    "def _to_device(batch, device):\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def _should_sync(device: str) -> bool:\n",
    "    return device.startswith(\"cuda\") and torch.cuda.is_available()\n",
    "\n",
    "def measure_latency(hf_model, batch, repeats = 200, warmup = 20, device =\"cpu\"):\n",
    "    hf_model.eval().to(device)\n",
    "    batch = _to_device(batch, device)\n",
    "    \n",
    "    # warmup (not timed)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(warmup):\n",
    "            _ = hf_model(**batch)\n",
    "    if _should_sync(device):\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    # timed loop\n",
    "    samples_ms = []\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = hf_model(**batch)\n",
    "            if _should_sync(device):\n",
    "                torch.cuda.synchronize()\n",
    "            samples_ms.append((time.perf_counter() - t0) * 1000.0)\n",
    "\n",
    "    samples_ms = np.asarray(samples_ms, dtype=float)\n",
    "    return {\n",
    "        \"batch_size\": int(batch[\"input_ids\"].shape[0]),\n",
    "        \"mean_ms\": float(samples_ms.mean()),\n",
    "        \"p50_ms\": float(np.percentile(samples_ms, 50)),\n",
    "        \"p95_ms\": float(np.percentile(samples_ms, 95)),\n",
    "        \"p99_ms\": float(np.percentile(samples_ms, 99)),\n",
    "        \"repeats\": int(repeats),\n",
    "    }\n",
    "    \n",
    "# grab a validation batch\n",
    "val_loader = dm.val_dataloader()\n",
    "batch_val = next(iter(val_loader))\n",
    "\n",
    "# bs=8\n",
    "fp32_bs8 = measure_latency(model.model, batch_val, repeats=150, warmup=30, device=\"cpu\")\n",
    "\n",
    "# bs=1 (edge/onboard-ish)\n",
    "single = {k: v[:1].clone() for k, v in batch_val.items()}\n",
    "fp32_bs1 = measure_latency(model.model, single, repeats=300, warmup=50, device=\"cpu\")\n",
    "\n",
    "print(\"FP32 baseline (bs=8):\", fp32_bs8)\n",
    "print(\"FP32 baseline (bs=1):\", fp32_bs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcde7ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fiona\\AppData\\Local\\Temp\\ipykernel_32672\\810471590.py:3: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qmodel = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 quant (bs=8): {'batch_size': 32, 'mean_ms': 3.5069446661509573, 'p50_ms': 3.36169998627156, 'p95_ms': 4.2835799453314385, 'p99_ms': 5.731635066913436, 'repeats': 150}\n",
      "INT8 quant (bs=1): {'batch_size': 1, 'mean_ms': 3.549782671422387, 'p50_ms': 3.6640000180341303, 'p95_ms': 4.545009910361841, 'p99_ms': 4.901293938746674, 'repeats': 300}\n",
      "Quick val accuracy FP32: 0.495 | INT8: 0.495\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "qmodel = torch.quantization.quantize_dynamic(\n",
    "    model.model, \n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "\n",
    "q_bs8 = measure_latency(qmodel, batch_val, repeats=150, warmup=30, device=\"cpu\")\n",
    "q_bs1 = measure_latency(qmodel, single,   repeats=300, warmup=50, device=\"cpu\")\n",
    "\n",
    "\n",
    "print(\"INT8 quant (bs=8):\", q_bs8)\n",
    "print(\"INT8 quant (bs=1):\", q_bs1)\n",
    "\n",
    "# quick accuracy sanity on a few batches\n",
    "def quick_accuracy(hf_model, loader, max_batches=10, device=\"cpu\"):\n",
    "    hf_model.eval().to(device)\n",
    "    correct = total = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, b in enumerate(loader):\n",
    "            if i >= max_batches: break\n",
    "            b = _to_device(b, device)\n",
    "            out = hf_model(**b)\n",
    "            preds = out.logits.argmax(dim=-1)\n",
    "            correct += (preds == b[\"labels\"]).sum().item()\n",
    "            total   += preds.numel()\n",
    "    return correct / total\n",
    "\n",
    "acc_fp32 = quick_accuracy(model.model, dm.val_dataloader(), max_batches=10)\n",
    "acc_int8 = quick_accuracy(qmodel,      dm.val_dataloader(), max_batches=10)\n",
    "print(f\"Quick val accuracy FP32: {acc_fp32:.3f} | INT8: {acc_int8:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc65bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA FP (bs=8): {'batch_size': 32, 'mean_ms': 1.9203689997084439, 'p50_ms': 1.893299981020391, 'p95_ms': 2.1348749636672437, 'p99_ms': 2.5890200270805495, 'repeats': 200}\n",
      "CUDA FP (bs=1): {'batch_size': 1, 'mean_ms': 1.8396346647447597, 'p50_ms': 1.7695500864647329, 'p95_ms': 2.163690055022016, 'p99_ms': 3.2102429785300024, 'repeats': 300}\n"
     ]
    }
   ],
   "source": [
    "fp32_cuda_bs8 = measure_latency(model.model, batch_val, repeats=200, warmup=60, device=\"cuda\")\n",
    "single = {k: v[:1].clone() for k, v in batch_val.items()}\n",
    "fp32_cuda_bs1 = measure_latency(model.model, single, repeats=300, warmup=80, device=\"cuda\")\n",
    "print(\"CUDA FP (bs=8):\", fp32_cuda_bs8)\n",
    "print(\"CUDA FP (bs=1):\", fp32_cuda_bs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5485a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray\n",
      "  Downloading ray-2.49.1-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting click>=7.0 (from ray)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray) (3.13.1)\n",
      "Collecting jsonschema (from ray)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray)\n",
      "  Downloading msgpack-1.1.1-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray) (25.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray) (6.32.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray) (2.32.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from click>=7.0->ray) (0.4.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from jsonschema->ray) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema->ray)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->ray)\n",
      "  Downloading rpds_py-0.27.1-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from referencing>=0.28.4->jsonschema->ray) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray) (2025.8.3)\n",
      "Downloading ray-2.49.1-cp312-cp312-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/26.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/26.2 MB 2.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.6/26.2 MB 2.6 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.1/26.2 MB 2.6 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.6/26.2 MB 2.6 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 3.1/26.2 MB 2.6 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 3.7/26.2 MB 2.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 4.2/26.2 MB 2.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 4.7/26.2 MB 2.6 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 5.5/26.2 MB 2.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 6.0/26.2 MB 2.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 6.6/26.2 MB 2.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 7.1/26.2 MB 2.6 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 7.6/26.2 MB 2.6 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 8.1/26.2 MB 2.6 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 8.7/26.2 MB 2.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 9.4/26.2 MB 2.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 10.0/26.2 MB 2.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 10.5/26.2 MB 2.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 11.0/26.2 MB 2.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.5/26.2 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 12.1/26.2 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 12.6/26.2 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 13.1/26.2 MB 2.6 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.9/26.2 MB 2.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.4/26.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.9/26.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 15.5/26.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 16.0/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.5/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 17.0/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 17.6/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 18.1/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 19.4/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.9/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 20.4/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 21.0/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 21.5/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.5/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.1/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.6/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 24.4/26.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.9/26.2 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.4/26.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 2.6 MB/s  0:00:10\n",
      "Downloading msgpack-1.1.1-cp312-cp312-win_amd64.whl (72 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.27.1-cp312-cp312-win_amd64.whl (232 kB)\n",
      "Installing collected packages: rpds-py, msgpack, click, referencing, jsonschema-specifications, jsonschema, ray\n",
      "\n",
      "   ----------- ---------------------------- 2/7 [click]\n",
      "   ----------- ---------------------------- 2/7 [click]\n",
      "   ----------- ---------------------------- 2/7 [click]\n",
      "   ----------- ---------------------------- 2/7 [click]\n",
      "   ----------------- ---------------------- 3/7 [referencing]\n",
      "   ----------------- ---------------------- 3/7 [referencing]\n",
      "   ---------------------- ----------------- 4/7 [jsonschema-specifications]\n",
      "   ---------------------------- ----------- 5/7 [jsonschema]\n",
      "   ---------------------------- ----------- 5/7 [jsonschema]\n",
      "   ---------------------------- ----------- 5/7 [jsonschema]\n",
      "   ---------------------------- ----------- 5/7 [jsonschema]\n",
      "   ---------------------------- ----------- 5/7 [jsonschema]\n",
      "   ---------------------------- ----------- 5/7 [jsonschema]\n",
      "   ---------------------------- ----------- 5/7 [jsonschema]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------- ----- 6/7 [ray]\n",
      "   ---------------------------------------- 7/7 [ray]\n",
      "\n",
      "Successfully installed click-8.2.1 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 msgpack-1.1.1 ray-2.49.1 referencing-0.36.2 rpds-py-0.27.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2320a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[train] in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (2.49.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (8.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (3.13.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (4.25.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (25.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (6.32.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (2.32.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (2.3.2)\n",
      "Collecting tensorboardX>=1.9 (from ray[train])\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (21.0.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from ray[train]) (2024.6.1)\n",
      "Collecting pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 (from ray[train])\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[train])\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[train])\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[train]) (4.12.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[train])\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from click>=7.0->ray[train]) (0.4.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from tensorboardX>=1.9->ray[train]) (2.1.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from jsonschema->ray[train]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from jsonschema->ray[train]) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from jsonschema->ray[train]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from jsonschema->ray[train]) (0.27.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from pandas->ray[train]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from pandas->ray[train]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from pandas->ray[train]) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->ray[train]) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray[train]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray[train]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray[train]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fiona\\documents\\github\\transformers\\transformers-mini\\.venv312\\lib\\site-packages (from requests->ray[train]) (2025.8.3)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/2.0 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.6 MB/s  0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tensorboardX, pydantic-core, annotated-types, pydantic\n",
      "\n",
      "   -------- ------------------------------- 1/5 [tensorboardX]\n",
      "   -------- ------------------------------- 1/5 [tensorboardX]\n",
      "   -------- ------------------------------- 1/5 [tensorboardX]\n",
      "   -------- ------------------------------- 1/5 [tensorboardX]\n",
      "   -------- ------------------------------- 1/5 [tensorboardX]\n",
      "   -------- ------------------------------- 1/5 [tensorboardX]\n",
      "   -------- ------------------------------- 1/5 [tensorboardX]\n",
      "   ---------------- ----------------------- 2/5 [pydantic-core]\n",
      "   ------------------------ --------------- 3/5 [annotated-types]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   -------------------------------- ------- 4/5 [pydantic]\n",
      "   ---------------------------------------- 5/5 [pydantic]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 pydantic-2.11.7 pydantic-core-2.33.2 tensorboardX-2.6.4 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Try the extras (often fine on 3.12). If this errors, skip itâ€”base `ray` is enough.\n",
    "%pip install -U \"ray[train]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "524d9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:08:07,474\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray: 2.49.1 | Python: 3.12.10 | OS: Windows-11-10.0.26100-SP0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:08:10,215\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squares: [0, 1, 4, 9, 16]\n",
      "Counter: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import ray, platform, sys\n",
    "print(\"Ray:\", ray.__version__, \"| Python:\", sys.version.split()[0], \"| OS:\", platform.platform())\n",
    "\n",
    "ray.init(ignore_reinit_error=True)  # starts a local Ray runtime\n",
    "\n",
    "@ray.remote\n",
    "def square(x): \n",
    "    return x * x\n",
    "\n",
    "futures = [square.remote(i) for i in range(5)]\n",
    "print(\"Squares:\", ray.get(futures))\n",
    "\n",
    "# (Optional) tiny actor test\n",
    "@ray.remote\n",
    "class Counter:\n",
    "    def __init__(self): self.n = 0\n",
    "    def inc(self): self.n += 1; return self.n\n",
    "\n",
    "c = Counter.remote()\n",
    "print(\"Counter:\", ray.get([c.inc.remote() for _ in range(3)]))\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f5e599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:08:12,922\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-06 23:08:13,001\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray Train OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from ray.train.torch import TorchTrainer\n",
    "    from ray.train import ScalingConfig\n",
    "    print(\"Ray Train OK\")\n",
    "except Exception as e:\n",
    "    print(\"Ray Train not available:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8835b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataModule (CUDA-ready) defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 1 â€” CUDA-friendly DataModule (workers/pinning)\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# keep your existing MODEL_ID / MAX_LEN from before\n",
    "# MODEL_ID = \"...\" \n",
    "# MAX_LEN = 128\n",
    "\n",
    "class SST2DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, model_id=MODEL_ID, batch_size=32, num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=2):\n",
    "        super().__init__()\n",
    "        self.model_id = model_id\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.collate = DataCollatorWithPadding(self.tokenizer)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        load_dataset(\"glue\", \"sst2\")\n",
    "        AutoTokenizer.from_pretrained(self.model_id)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        ds_train = load_dataset(\"glue\", \"sst2\", split=\"train[:1000]\")\n",
    "        ds_val   = load_dataset(\"glue\", \"sst2\", split=\"validation[:200]\")\n",
    "\n",
    "        def tok_fn(batch):\n",
    "            t = self.tokenizer(batch[\"sentence\"], truncation=True, max_length=MAX_LEN)\n",
    "            t[\"labels\"] = batch[\"label\"]\n",
    "            return t\n",
    "\n",
    "        ds_train = ds_train.map(tok_fn, batched=True, remove_columns=ds_train.column_names)\n",
    "        ds_val   = ds_val.map(tok_fn,   batched=True, remove_columns=ds_val.column_names)\n",
    "        self.ds_train, self.ds_val = ds_train, ds_val\n",
    "\n",
    "    def _loader(self, ds, shuffle: bool):\n",
    "        kw = dict(\n",
    "            dataset=ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.collate,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,\n",
    "        )\n",
    "        if self.num_workers > 0:\n",
    "            kw[\"prefetch_factor\"] = self.prefetch_factor\n",
    "        return DataLoader(**kw)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._loader(self.ds_train, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._loader(self.ds_val, shuffle=False)\n",
    "\n",
    "print(\"DataModule (CUDA-ready) defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47e6964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 16523.48 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 6877.43 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sshleifer/tiny-distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 17056.12 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 4441.99 examples/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                             | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | model | RobertaForSequenceClassification | 101 K  | eval\n",
      "------------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "50        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:527: Found 50 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 18.64it/s, v_num=3, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 18.48it/s, v_num=3, val_loss=0.693, val_acc=0.495, train_loss=0.693, train_acc=0.542]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” GPU training\n",
    "import pytorch_lightning as pl, torch\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "dm = SST2DataModule(model_id=MODEL_ID, batch_size=32, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "dm.prepare_data(); dm.setup()\n",
    "\n",
    "model = LitTinyClassifier(model_id=MODEL_ID, lr=5e-5)\n",
    "\n",
    "can_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "precision = \"bf16-mixed\" if can_bf16 else (\"16-mixed\" if torch.cuda.is_available() else \"32-true\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=precision,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ccda7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA FP32 (bs=8): {'batch_size': 32, 'mean_ms': 2.1199090004665777, 'p50_ms': 1.9768000347539783, 'p95_ms': 3.5530599532648877, 'p99_ms': 3.999122950481251, 'repeats': 200, 'amp': False, 'device': 'cuda'}\n",
      "CUDA AMP  (bs=8): {'batch_size': 32, 'mean_ms': 2.750707999803126, 'p50_ms': 2.607349946629256, 'p95_ms': 3.9444049878511573, 'p99_ms': 5.274887993000447, 'repeats': 200, 'amp': True, 'device': 'cuda'}\n",
      "CUDA FP32 (bs=1): {'batch_size': 1, 'mean_ms': 2.2397536667995155, 'p50_ms': 1.9745000172406435, 'p95_ms': 3.5744099703151733, 'p99_ms': 3.9626729849260256, 'repeats': 300, 'amp': False, 'device': 'cuda'}\n",
      "CUDA AMP  (bs=1): {'batch_size': 1, 'mean_ms': 2.769810004004588, 'p50_ms': 2.556700026616454, 'p95_ms': 4.814254981465638, 'p99_ms': 5.666363951750099, 'repeats': 300, 'amp': True, 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” CUDA latency\n",
    "import time, numpy as np, torch\n",
    "\n",
    "def _to_device(batch, device):\n",
    "    return {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "def _should_sync(device: str) -> bool:\n",
    "    return device.startswith(\"cuda\") and torch.cuda.is_available()\n",
    "\n",
    "def measure_latency(hf_model, batch, repeats=200, warmup=60, device=\"cuda\", amp=False):\n",
    "    hf_model.eval().to(device)\n",
    "    batch = _to_device(batch, device)\n",
    "\n",
    "    # warmup\n",
    "    with torch.inference_mode():\n",
    "        if amp and device.startswith(\"cuda\"):\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)):\n",
    "                for _ in range(warmup): _ = hf_model(**batch)\n",
    "        else:\n",
    "            for _ in range(warmup): _ = hf_model(**batch)\n",
    "    if _should_sync(device): torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            if amp and device.startswith(\"cuda\"):\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)):\n",
    "                    _ = hf_model(**batch)\n",
    "            else:\n",
    "                _ = hf_model(**batch)\n",
    "            if _should_sync(device): torch.cuda.synchronize()\n",
    "            times.append((time.perf_counter() - t0) * 1000.0)\n",
    "\n",
    "    arr = np.asarray(times, dtype=float)\n",
    "    return {\n",
    "        \"batch_size\": int(batch[\"input_ids\"].shape[0]),\n",
    "        \"mean_ms\": float(arr.mean()),\n",
    "        \"p50_ms\": float(np.percentile(arr, 50)),\n",
    "        \"p95_ms\": float(np.percentile(arr, 95)),\n",
    "        \"p99_ms\": float(np.percentile(arr, 99)),\n",
    "        \"repeats\": int(repeats),\n",
    "        \"amp\": bool(amp),\n",
    "        \"device\": device,\n",
    "    }\n",
    "\n",
    "# grab a val batch\n",
    "val_loader = dm.val_dataloader()\n",
    "batch_val = next(iter(val_loader))\n",
    "\n",
    "# bs=8\n",
    "fp32_cuda_bs8 = measure_latency(model.model, batch_val, repeats=200, warmup=80, device=\"cuda\", amp=False)\n",
    "amp_cuda_bs8  = measure_latency(model.model, batch_val, repeats=200, warmup=80, device=\"cuda\", amp=True)\n",
    "\n",
    "# bs=1\n",
    "single = {k: v[:1].clone() for k, v in batch_val.items()}\n",
    "fp32_cuda_bs1 = measure_latency(model.model, single, repeats=300, warmup=100, device=\"cuda\", amp=False)\n",
    "amp_cuda_bs1  = measure_latency(model.model, single, repeats=300, warmup=100, device=\"cuda\", amp=True)\n",
    "\n",
    "print(\"CUDA FP32 (bs=8):\", fp32_cuda_bs8)\n",
    "print(\"CUDA AMP  (bs=8):\", amp_cuda_bs8)\n",
    "print(\"CUDA FP32 (bs=1):\", fp32_cuda_bs1)\n",
    "print(\"CUDA AMP  (bs=1):\", amp_cuda_bs1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10a2a169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\nn\\functional.py:2905: UserWarning: record_context_cpp is not support on non-linux non-x86_64 platforms (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\profiler\\unwind\\unwind.cpp:12.)\n",
      "  return torch.layer_norm(\n",
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0906 23:13:26.596000 32672 Lib\\site-packages\\torch\\_inductor\\utils.py:1436] [0/0_1] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.compile not usable here: TritonMissing('Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\\n\\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you\\'re reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fiona\\Documents\\GitHub\\Transformers\\transformers-mini\\.venv312\\Lib\\site-packages\\torch\\_inductor\\lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 â€” torch.compile (inference)\n",
    "import torch\n",
    "if hasattr(torch, \"compile\"):\n",
    "    try:\n",
    "        cmodel = torch.compile(model.model, dynamic=True)\n",
    "        c_bs8 = measure_latency(cmodel, batch_val, repeats=200, warmup=120, device=\"cuda\", amp=False)\n",
    "        c_bs1 = measure_latency(cmodel, single,   repeats=300, warmup=150, device=\"cuda\", amp=False)\n",
    "        print(\"CUDA compile FP32 (bs=8):\", c_bs8)\n",
    "        print(\"CUDA compile FP32 (bs=1):\", c_bs1)\n",
    "    except Exception as e:\n",
    "        print(\"torch.compile not usable here:\", repr(e))\n",
    "else:\n",
    "    print(\"torch.compile not available in this build.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 venv",
   "language": "python",
   "name": "pt312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
