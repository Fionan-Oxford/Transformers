{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bbd3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 3911, 'target': {1: np.int64(1564), 2: np.int64(1564), 3: np.int64(313), 4: np.int64(313), 5: np.int64(157)}, 'train': {1: 1251, 2: 1251, 3: 250, 4: 250, 5: 126}, 'val': {1: 156, 2: 156, 3: 31, 4: 32, 5: 16}, 'test': {1: 157, 2: 157, 3: 32, 4: 31, 5: 15}}\n"
     ]
    }
   ],
   "source": [
    "# make_binned_splits.py\n",
    "# Creates train.csv / val.csv / test.csv with headers: content, score (1..5)\n",
    "# - Bins are built from the average of the six traits\n",
    "# - You control target proportions per bin via `target_props`\n",
    "\n",
    "import os, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, cohen_kappa_score\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG: tweak these freely\n",
    "# --------------------------\n",
    "binning = \"quantile\"              # \"quantile\" or \"fixed\"\n",
    "n_bins = 5                        # number of bins/classes (1..5)\n",
    "# If you choose \"fixed\", provide n_bins+1 edges covering the min..max of averages.\n",
    "# Example below roughly centers around the mean ~3:\n",
    "fixed_edges = [1.0, 2.2, 2.8, 3.3, 3.8, 5.01]  # inclusive-lowest; last edge should exceed max\n",
    "\n",
    "# Desired class proportions (must sum to 1.0). Example: equal bins.\n",
    "target_props = {1: 0.40, 2: 0.40, 3: 0.08, 4: 0.08, 5: 0.04}\n",
    "# Another example (intentionally unbalanced):\n",
    "# target_props = {1: 0.10, 2: 0.20, 3: 0.40, 4: 0.20, 5: 0.10}\n",
    "\n",
    "subset_fraction = 1.0             # use 1.0 for all data; <1.0 to take a random subset first\n",
    "oversample = True                 # if a bin is short, sample with replacement to hit target proportion\n",
    "random_state = 42                 # reproducibility\n",
    "train_frac, val_frac, test_frac = 0.80, 0.10, 0.10\n",
    "\n",
    "# --------------------------\n",
    "# Load & prep\n",
    "# --------------------------\n",
    "ds = load_dataset(\"tasksource/english-grading\", split=\"train\")\n",
    "df = ds.to_pandas()\n",
    "\n",
    "traits = [\"cohesion\",\"syntax\",\"vocabulary\",\"phraseology\",\"grammar\",\"conventions\"]\n",
    "df[\"avg\"] = df[traits].mean(axis=1)\n",
    "\n",
    "# Bin the averages -> integer scores 1..5\n",
    "if binning == \"quantile\":\n",
    "    # equal-frequency bins for a more even distribution\n",
    "    # duplicates=\"drop\" guards against pathological ties collapsing bins\n",
    "    binned = pd.qcut(df[\"avg\"], q=n_bins, labels=range(1, n_bins + 1), duplicates=\"drop\")\n",
    "    # If for some reason fewer than n_bins were created (rare), fall back to fixed-width bins over observed range\n",
    "    if binned.isna().any() or len(binned.cat.categories) < n_bins:\n",
    "        # Build fixed-width edges across the data range\n",
    "        edges = np.linspace(df[\"avg\"].min() - 1e-6, df[\"avg\"].max() + 1e-6, n_bins + 1)\n",
    "        binned = pd.cut(df[\"avg\"], bins=edges, labels=range(1, n_bins + 1), include_lowest=True)\n",
    "    df[\"score\"] = binned.astype(int)\n",
    "elif binning == \"fixed\":\n",
    "    edges = fixed_edges\n",
    "    assert len(edges) == n_bins + 1, \"fixed_edges must have n_bins+1 numbers\"\n",
    "    df[\"score\"] = pd.cut(df[\"avg\"], bins=edges, labels=range(1, n_bins + 1), include_lowest=True).astype(int)\n",
    "else:\n",
    "    raise ValueError(\"binning must be 'quantile' or 'fixed'\")\n",
    "\n",
    "# Use only the two requested columns, dedupe on content\n",
    "two_col = pd.DataFrame({\"content\": df[\"full_text\"], \"score\": df[\"score\"].astype(int)}).drop_duplicates(\"content\")\n",
    "\n",
    "# Optionally take a random subset first (before proportioning)\n",
    "if subset_fraction < 1.0:\n",
    "    two_col = two_col.sample(frac=subset_fraction, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# Build a dataset to match target proportions\n",
    "# --------------------------\n",
    "# Normalize/validate proportions\n",
    "keys = sorted(target_props.keys())\n",
    "assert keys == list(range(1, n_bins + 1)), f\"target_props must have keys 1..{n_bins}\"\n",
    "props = np.array([target_props[k] for k in keys], dtype=float)\n",
    "props = props / props.sum()\n",
    "\n",
    "total = len(two_col)\n",
    "# Compute exact desired counts using largest remainder method\n",
    "raw = props * total\n",
    "base = np.floor(raw).astype(int)\n",
    "remainder = total - base.sum()\n",
    "# Distribute the remaining items to classes with largest fractional parts\n",
    "order = np.argsort(-(raw - base))  # descending by fractional part\n",
    "for i in range(remainder):\n",
    "    base[order[i]] += 1\n",
    "desired_counts = dict(zip(keys, base))\n",
    "\n",
    "# Sample per class\n",
    "rng = np.random.default_rng(random_state)\n",
    "parts = []\n",
    "for cls, n_needed in desired_counts.items():\n",
    "    group = two_col[two_col[\"score\"] == cls]\n",
    "    if len(group) == 0:\n",
    "        continue  # no examples of this class available\n",
    "    replace = oversample and (n_needed > len(group))\n",
    "    sampled = group.sample(n=min(n_needed, len(group)) if not replace else n_needed,\n",
    "                           replace=replace, random_state=random_state)\n",
    "    parts.append(sampled)\n",
    "\n",
    "balanced = pd.concat(parts, axis=0).sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# Stratified train/val/test split (preserves your chosen proportions)\n",
    "# --------------------------\n",
    "train_df, temp_df = train_test_split(\n",
    "    balanced, test_size=(1 - train_frac), stratify=balanced[\"score\"], random_state=random_state\n",
    ")\n",
    "relative_test = test_frac / (val_frac + test_frac)  # split temp into val/test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=relative_test, stratify=temp_df[\"score\"], random_state=random_state\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Save CSVs\n",
    "# --------------------------\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "# Quick class balance report\n",
    "def counts(df):\n",
    "    return df[\"score\"].value_counts().sort_index().to_dict()\n",
    "print({\n",
    "    \"total\": len(balanced),\n",
    "    \"target\": desired_counts,\n",
    "    \"train\": counts(train_df),\n",
    "    \"val\": counts(val_df),\n",
    "    \"test\": counts(test_df),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"bert-tiny\", local_files_only=True, use_fast=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# inverse-frequency class weights (normalized to mean=1)\n",
    "cls_counts   = train_df[\"score\"].astype(int).value_counts().sort_index()\n",
    "inv          = 1.0 / cls_counts\n",
    "class_weight = (inv / inv.mean()).to_dict()\n",
    "\n",
    "\n",
    "class_weight_vec = torch.tensor(\n",
    "    [class_weight.get(k, 1.0) for k in [1,2,3,4,5]],\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "# DataLoaders that yield just indices; we fetch & chunk texts on-the-fly\n",
    "train_idx = torch.arange(len(train_df))\n",
    "val_idx   = torch.arange(len(val_df))\n",
    "\n",
    "w_train = torch.tensor(train_df[\"score\"].astype(int).map(class_weight).values, dtype=torch.double)\n",
    "sampler = WeightedRandomSampler(weights=w_train, num_samples=len(w_train), replacement=True)\n",
    "\n",
    "BATCH_ES = 8  # essays per batch (CPU-friendly)\n",
    "# shuffle \n",
    "#train_dl = DataLoader(TensorDataset(train_idx), batch_size=BATCH_ES, sampler=sampler)\n",
    "train_dl = DataLoader(TensorDataset(train_idx), batch_size=BATCH_ES, shuffle=True)\n",
    "\n",
    "val_dl   = DataLoader(TensorDataset(val_idx),   batch_size=BATCH_ES, shuffle=False)\n",
    "\n",
    "# chunking helpers\n",
    "MAX_LEN = 512\n",
    "STRIDE  = 128\n",
    "EPOCHS = 20\n",
    "\n",
    "def chunk_encode(text):\n",
    "    enc = tok(\n",
    "        text,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        stride=STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    if \"token_type_ids\" not in enc:\n",
    "        enc[\"token_type_ids\"] = torch.zeros_like(enc[\"input_ids\"])\n",
    "    return enc\n",
    "\n",
    "@torch.no_grad()\n",
    "def chunk_predict_mean(model, text):\n",
    "    enc = chunk_encode(text)\n",
    "    out = model(\n",
    "        input_ids=enc[\"input_ids\"].to(device),\n",
    "        attention_mask=enc[\"attention_mask\"].to(device),\n",
    "        token_type_ids=enc[\"token_type_ids\"].to(device),\n",
    "    ).logits.squeeze(-1)\n",
    "    return out.mean().item()\n",
    "\n",
    "def clip_round(x): return np.clip(np.rint(x), 1, 5).astype(int)\n",
    "def score_to_class(s):  # scores 1..5 → class ids 0..4\n",
    "    return int(round(float(s))) - 1\n",
    "\n",
    "def class_to_score(c):  # class ids 0..4 → scores 1..5\n",
    "    return int(c) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb53f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-tiny and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1, 128]) in the checkpoint and torch.Size([5, 128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|██████████| 391/391 [01:41<00:00,  3.84it/s, loss=1.5810]\n",
      "Epoch 2/10: 100%|██████████| 391/391 [01:11<00:00,  5.50it/s, loss=1.5731]\n",
      "Epoch 3/10: 100%|██████████| 391/391 [01:11<00:00,  5.44it/s, loss=2.0370]\n",
      "Epoch 4/10: 100%|██████████| 391/391 [01:10<00:00,  5.52it/s, loss=1.1719]\n",
      "Epoch 5/10: 100%|██████████| 391/391 [01:10<00:00,  5.54it/s, loss=2.2015]\n",
      "Epoch 6/10: 100%|██████████| 391/391 [01:11<00:00,  5.49it/s, loss=2.4405]\n",
      "Epoch 7/10: 100%|██████████| 391/391 [01:11<00:00,  5.47it/s, loss=1.8244]\n",
      "Epoch 8/10: 100%|██████████| 391/391 [01:11<00:00,  5.50it/s, loss=1.1583]\n",
      "Epoch 9/10: 100%|██████████| 391/391 [01:11<00:00,  5.49it/s, loss=1.1122]\n",
      "Epoch 10/10: 100%|██████████| 391/391 [01:10<00:00,  5.53it/s, loss=0.9762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_acc': 0.6138, 'QWK': 0.2884, 'target_met_≥75%': False}\n",
      "Saved to finetuned_bert_tiny_chunked\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# model & opt\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-tiny\", local_files_only=True, num_labels=5,ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_training_steps = len(train_dl) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=max(1, num_training_steps//10), num_training_steps=num_training_steps\n",
    ")\n",
    "mse = nn.MSELoss(reduction=\"none\")\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weight_vec, label_smoothing=0.05)\n",
    "\n",
    "# ---- train (1 epoch), chunking + mean aggregation ----\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for (idx_batch,) in pbar:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch_size = len(idx_batch)\n",
    "        for idx in idx_batch.tolist():\n",
    "            row   = train_df.iloc[idx]\n",
    "            text  = row[\"content\"]\n",
    "            target = torch.tensor(score_to_class(row[\"score\"]), dtype=torch.long, device=device)\n",
    "\n",
    "            enc = tok(\n",
    "                text,\n",
    "                max_length=512, truncation=True, stride=STRIDE,\n",
    "                return_overflowing_tokens=True, return_tensors=\"pt\",\n",
    "                return_token_type_ids=True, padding=\"max_length\",\n",
    "            )\n",
    "            if \"token_type_ids\" not in enc:\n",
    "                enc[\"token_type_ids\"] = torch.zeros_like(enc[\"input_ids\"])\n",
    "\n",
    "            # logits per chunk: (n_chunks, 5)\n",
    "            logits_chunks = model(\n",
    "                input_ids=enc[\"input_ids\"].to(device),\n",
    "                attention_mask=enc[\"attention_mask\"].to(device),\n",
    "                token_type_ids=enc[\"token_type_ids\"].to(device),\n",
    "            ).logits\n",
    "\n",
    "            # lightly weight later chunks (optional); mean is also fine\n",
    "            n = logits_chunks.shape[0]\n",
    "            w = torch.linspace(0.9, 1.1, steps=n, device=logits_chunks.device).unsqueeze(1)\n",
    "            logits_essay = (logits_chunks * w).sum(dim=0) / w.sum()  # shape (5,)\n",
    "\n",
    "            loss_i = criterion(logits_essay.unsqueeze(0), target.unsqueeze(0))\n",
    "            (loss_i / batch_size).backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pbar.set_postfix(loss=f\"{loss_i.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ---- validate (chunked) ----\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for (idx_batch,) in val_dl:\n",
    "        for idx in idx_batch.tolist():\n",
    "            row   = val_df.iloc[idx]\n",
    "            text  = row[\"content\"]\n",
    "            true_score = int(round(row[\"score\"]))\n",
    "\n",
    "            enc = tok(\n",
    "                text,\n",
    "                max_length=MAX_LEN, truncation=True, stride=STRIDE,\n",
    "                return_overflowing_tokens=True, return_tensors=\"pt\",\n",
    "                return_token_type_ids=True, padding=\"max_length\",\n",
    "            )\n",
    "            if \"token_type_ids\" not in enc:\n",
    "                enc[\"token_type_ids\"] = torch.zeros_like(enc[\"input_ids\"])\n",
    "\n",
    "            logits_chunks = model(\n",
    "                input_ids=enc[\"input_ids\"].to(device),\n",
    "                attention_mask=enc[\"attention_mask\"].to(device),\n",
    "                token_type_ids=enc[\"token_type_ids\"].to(device),\n",
    "            ).logits  # (n_chunks, 5)\n",
    "\n",
    "            n = logits_chunks.shape[0]\n",
    "            w = torch.linspace(0.9, 1.1, steps=n, device=logits_chunks.device).unsqueeze(1)\n",
    "            logits_essay = (logits_chunks * w).sum(dim=0) / w.sum()\n",
    "\n",
    "            pred_class = int(torch.argmax(logits_essay).item())  # 0..4\n",
    "            pred_score = class_to_score(pred_class)              # 1..5\n",
    "\n",
    "            preds.append(pred_score)\n",
    "            trues.append(true_score)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "preds = np.array(preds, dtype=int); trues = np.array(trues, dtype=int)\n",
    "acc = accuracy_score(trues, preds)\n",
    "qwk = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n",
    "print({\"val_acc\": round(acc,4), \"QWK\": round(qwk,4), \"target_met_≥75%\": acc >= 0.75})\n",
    "\n",
    "\n",
    "# ---- save model + tokenizer + optimizer/scheduler ----\n",
    "SAVE_DIR = \"finetuned_bert_tiny_chunked\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tok.save_pretrained(SAVE_DIR)\n",
    "torch.save({\"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"class_weight\": class_weight},\n",
    "           os.path.join(SAVE_DIR, \"optimizer_scheduler.pt\"))\n",
    "print(\"Saved to\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "150908f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer (chunked): 100%|██████████| 49/49 [00:02<00:00, 19.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submissions.csv with 392 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_DIR = \"finetuned_bert_tiny_chunked\"   # ← adjust if you used a different save path\n",
    "TEST_CSV  = \"test.csv\"\n",
    "OUT_CSV   = \"submissions.csv\"\n",
    "\n",
    "# ---- load test data ----\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "assert \"content\" in test_df.columns, \"test.csv must have a 'content' column\"\n",
    "test_df = test_df.dropna(subset=[\"content\"]).copy()\n",
    "test_df[\"content\"] = test_df[\"content\"].astype(str).str.strip()\n",
    "\n",
    "# ---- load tokenizer & model ----\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True, use_fast=True)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ---- chunking helpers ----\n",
    "MAX_LEN, STRIDE = 512, 128\n",
    "def chunk_encode(text):\n",
    "    enc = tok(\n",
    "        text,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        stride=STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    if \"token_type_ids\" not in enc:\n",
    "        enc[\"token_type_ids\"] = torch.zeros_like(enc[\"input_ids\"])\n",
    "    return enc\n",
    "\n",
    "@torch.no_grad()\n",
    "def chunk_predict_mean(text):\n",
    "    enc = chunk_encode(text)\n",
    "    out = model(\n",
    "        input_ids=enc[\"input_ids\"].to(device),\n",
    "        attention_mask=enc[\"attention_mask\"].to(device),\n",
    "        token_type_ids=enc[\"token_type_ids\"].to(device),\n",
    "    ).logits.squeeze(-1)\n",
    "    return out.mean().item()\n",
    "\n",
    "def clip_round(x): return np.clip(np.rint(x), 1, 5).astype(int)\n",
    "\n",
    "# ---- DataLoader of indices (fetch text on the fly) ----\n",
    "idx = torch.arange(len(test_df))\n",
    "test_dl = DataLoader(TensorDataset(idx), batch_size=8, shuffle=False)\n",
    "\n",
    "# ---- inference ----\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for (idx_batch,) in tqdm(test_dl, desc=\"Infer (chunked)\"):\n",
    "        for i in idx_batch.tolist():\n",
    "            preds.append(chunk_predict_mean(test_df.iloc[i][\"content\"]))\n",
    "\n",
    "preds = np.array(preds, dtype=np.float32)\n",
    "preds_int = clip_round(preds)\n",
    "\n",
    "# ---- save submissions ----\n",
    "out = pd.DataFrame({\n",
    "    \"content\": test_df[\"content\"].values,\n",
    "    \"score\": preds_int\n",
    "})\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved {OUT_CSV} with {len(out)} rows.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (rivian-pt312)",
   "language": "python",
   "name": "rivian-pt312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
